{% extends "layout.html" %}

{% block title %}Model: Bayesian Optimisation for Biology{% endblock %}
{% block lead %}{% endblock %}

{% block scroll_nav %}
<nav class="side-scroll-nav scroll-nav" style="font-size: 0.7rem; width: 180px;">
  <span class="side-scroll-nav-label">On This Page</span>
  <a class="span-lime-green" href="#results"><span>Results</span></a>
  <a class="span-lime-green" href="#what-is-bo"><span>What is Bayesian Optimisation?</span></a>
  <a class="span-lime-green" href="#why-underemployed"><span>Why is Bayesian Optimisation Under-Employed in
      Biology?</span></a>
  <a class="span-lime-green" href="#outlook"><span>Outlook</span></a>
  <a class="span-lime-green" href="#references"><span>References</span></a>
</nav>
{% endblock %}

{% block page_content %}
<!-- HERO BANNER -->
<div class="education-hero hp-banner">
  <div class="education-hero-content-container">
    <h1 class="text-glitch">MODEL</h1>
  </div>
</div>

<div class="spacer5"></div>

<!-- INTRODUCTION -->
<div class="page-section width-75" style="margin-left: 220px;" id="introduction">
  <div class="section-title">
    <h1 style="text-align: left;" class="animate-text-scramble">Introduction</h1>
  </div>

  <p style="text-align: left;">
    During our experiments we recognised that having a readily accessible <strong>optimisation model</strong> would
    streamline decisions on <strong>media composition</strong>, <strong>incubation times</strong>, and the upkeep of
    both bioreactors and shake flasks. A review of scientific literature and past iGEM contributions suggested that
    a
    <strong>generalised, no-code batch Bayesian optimisation workflow</strong> would offer a distinctive capability
    to the community. This led us to develop <strong>BioKernel</strong>, our no-code interface for experiment
    optimisation (<a href="{{ url_for('pages', page='model') }}">explore the model</a>).
  </p>

  <p style="text-align: left;">
    Synthetic biology projects consistently face a fundamental challenge: how to achieve optimal system performance
    when experimental resources are severely constrained. During our project development, we confronted this reality
    directly. Our goal was to metabolically engineer our chassis into a high-performance production system, yet
    cloning complexity, protracted growth cycles, and limited lab infrastructure meant we could conduct only a
    handful of DBTL cycles before the project freeze. With access to just a few large shake flasks and a single
    bioreactor, yet dozens of strain modifications and culture conditions to explore, we needed a rigorous approach
    to <strong>extract maximum information from minimal experiments</strong>.
  </p>

  <p style="text-align: left;">
    This challenge extends far beyond our immediate project. Biological optimisation problems are fundamentally
    difficult: they involve expensive-to-evaluate objective functions, inherent experimental noise, particularly
    heteroscedastic noise which is non-constant, and high-dimensional design spaces[2]. Traditional approaches like
    exhaustive screening or one-factor-at-a-time experimentation are prohibitively resource-intensive. While
    Bayesian optimisation has emerged as a powerful solution for such scenarios, existing implementations often lack
    accessibility for experimental biologists or the flexibility to handle the specific complexities of biological
    data.
  </p>

  <p style="text-align: left;"> <strong>We developed BioKernel, a no-code Bayesian optimisation framework specifically
      designed
      to guide biological experimental campaigns toward optimal outcomes with minimal resource
      expenditure.</strong>  Our software addresses key limitations of existing tools through some critical
    innovations that are, to our best
    knowledge, novel to iGEM:</p>
  <ul class="feature-list" style="text-align:left; margin-left:1rem; line-height:1.6;">
    <li><strong>Modular kernel architecture</strong> — enabling users to select or combine covariance functions
      appropriate for their biological system.</li>
    <li><strong>Flexible acquisition function selection</strong> — Expected Improvement, Upper Confidence Bound,
      Probability of Improvement, etc., to balance exploration and exploitation based on experimental goals.</li>
    <li><strong>Heteroscedastic noise modelling</strong> — accurately captures the non-constant measurement
      uncertainty inherent in biological systems.</li>
    <li><strong>Support for variable batch sizes and technical replicates</strong> — recognises practical laboratory
      workflows and provides flexibility.</li>
  </ul>

  <p style="text-align: left;">These features transform Bayesian optimisation from a theory-heavy tool into a
    practical laboratory companion, enabling researchers to intelligently navigate complex parameter spaces and
    identify high-performing conditions with dramatically fewer experiments than conventional approaches.</p>
</div>




<!-- Validation Strategy and Broader Applicability -->

<div class="page-section width-75" style="margin-left: 220px;" id="validation-strategy">
  <div class="section-title">
    <h1 style="text-align: left;" class="span-lime-green">Validation Strategy and Broader Applicability</h1>
  </div>

  <p style="text-align: left;">To validate our framework, we pursued two complementary approaches. First, we applied
    our software to optimise published datasets from metabolic engineering studies, demonstrating that our approach
    successfully identifies optimal conditions with substantially fewer experiments than were originally required.
  </p>
  <p style="text-align: left;">Second, we designed a comprehensive experimental proof-of-concept: optimising
    astaxanthin production via a heterologous 10-step enzymatic pathway integrated into the Marionette-wild<i>E.
      coli</i> strain[5]. This strain possesses a genomically integrated array of twelve orthogonal, highly sensitive
    inducible transcription factors, allowing for a twelve-dimensional optimisation landscape ideal for
    demonstrating our software's capabilities. By systematically varying inducer concentrations across the pathway,
    we aim to verify that Bayesian optimisation could guide this complex, multi-step enzymatic process to a strong
    optimum using far fewer experiments than conventional screening methods.</p>

  <p style="text-align: left;">We opted to use astaxanthin as it is readily quantified spectrophotometrically[23],
    reducing the time needed to evaluate each batch. </p>
  <p style="text-align: left;">It is not economically feasible to utilise inducible promoters for industrial
    transcriptional control. We thus propose utilising the framework from[6], offering a solution to find a
    constitutive `match` for the expression levels corresponding to an optimum reached in an experimental campaign.
    This way, expensive inducers within the Marionette array, such as naringenin[5], are only necessary for initial
    screening campaigns. </p>
  <p style="text-align: left;">While parts delivery delays prevented completion of our full experimental validation
    within the competition timeline, the successful retrospective optimisation of published datasets serves as a
    compelling proof of concept. These results demonstrate that our framework can effectively optimise cellular
    "black box" functions, systems where the relationship between inputs and outputs is unknown, using substantially
    fewer experimental iterations than traditional approaches.</p>

</div>

<div class="spacer"></div>
<!-- RESULTS -->
<div class="page-section width-75" style="margin-left: 220px; margin-bottom: 3rem;" id="results">
  <h2 style="text-align: left;">
    <span class="span-lime-green" class="animate-text-scramble"><span>Results</span>
  </h2>

  <!-- DROPDOWN START -->
  <details style="margin-top: 1rem; margin-bottom: 2rem;">
    <summary style="cursor: pointer; font-weight: bold;">Show Details</summary>
    <div style="margin-top: 1rem;">

      <p style="text-align: left; margin-bottom: 1.5rem;">
        hough we could not perform optimisation batches using our marionette strain due to an unforeseen, significant
        order delay of the parts needed to clone the astaxanthin pathway, we validated our package on empirical data.
      </p>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        To trial the effectiveness of our package despite this delay, we took a dataset from an optimisation experiment
        applying four-dimensional transcriptional control to limonene production in Marionette-wild <i>Escherichia
          coli</i> [21]. Though
        this represents a significantly more tractable optimisation problem than the astaxanthin pathway we chose for
        our in-lab validation, it serves as strong validation that BioKernel can find an optimum far faster than the
        exhaustive combinatorial search.
      </p>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        As the dataset of this paper is relatively sparse, we fitted a Gaussian process with a scaled RBF kernel and
        additional white noise kernel to their data, creating a surface approximating the actual optimisation landscape
        of the four-dimensional input space. The procedure included training a mixed model of Random Forest (RF) and
        K-Nearest Neighbours (KNN) on 83 unique parameter combinations.
      </p>

      <div class="image-aligner-center" style="text-align: center; margin: 2rem 0;">
        <img src="https://static.igem.wiki/teams/5916/model/dry-signal.avif" alt="Gaussian process noise surface"
          style="max-width: 80%; border-radius: 0.5rem;" class="tilt-effect">
        <div style="text-align: center; margin-top: 0.5rem; color: #aaa;">
          <em>Figure 1. Extrapolated normalised Limonene production surface derived from 83 unique observations with six
            technical repeats
            each. Extrapolated using a mixed model of Random Forest and K-Nearest Neighbours</em>
        </div>
      </div>

      <p style="text-align: left;">The same procedure was applied, however instead of using the means calculated from
        experimental data, we
        estimated the noise by calculating the standard deviation. This noise was then used to build a heteroscedastic
        noise meshgrid, which was supplied as a standard deviation parameter for random sampling from a normal
        distribution. </p>

      <!-- Figure 1 -->
      <div class="image-aligner-center" style="text-align: center; margin: 2rem 0;">
        <img src="https://static.igem.wiki/teams/5916/model/dry-noise.avif" alt="Gaussian process noise surface"
          style="max-width: 80%; border-radius: 0.5rem;" class="tilt-effect">
        <div style="text-align: center; margin-top: 0.5rem; color: #aaa;">
          <em>Figure 2. Extrapolated noise surface derived from 83 unique observations with six technical repeats
            each. Extrapolated using a mixed model of Random Forest and K-Nearest Neighbours from standard deviation of
            each group</em>
        </div>
      </div>


      <p style="text-align: left;">This surface then became our test set for BioKernel. The following optimisation was
        performed using our
        package, with options set to use a Matern kernel with a gamma noise prior. </p>

      <p style="text-align: left;">BioKernel algorithm converged to the optimum as measured by normalised euclidean
        distance of [PLACEHOLDER] in
        four dimensions (maximal distance would be 2) within five sequential batches of six technical replicates. It
        thus took BioKernel [INSERT] measurements to converge close to the optimum, as opposed to the 648 taken by the
        adapted grid-search used in the paper[21]. </p>


    </div>
  </details>
  <!-- DROPDOWN END -->
</div>
<!-- WHAT IS BAYESIAN OPTIMISATION -->
<div class="page-section width-75" style="margin-left: 220px; margin-bottom: 3rem;" id="what-is-bo">
  <h2 style="text-align: left;">
    <span class="span-lime-green" class="animate-text-scramble"><span>What is Bayesian Optimisation?</span>
  </h2>

  <!-- DROPDOWN START -->
  <details style="margin-top: 1rem; margin-bottom: 2rem;">
    <summary style="cursor: pointer; font-weight: bold;">Show Details</summary>
    <div style="margin-top: 1rem;">

      <p style="text-align: left; margin-bottom: 1.5rem;">
        Bayesian Optimisation (BO) is a sample-efficient, sequential strategy for global optimisation of black-box
        functions[1]. In essence, it enables the identification of input parameter combinations that yield an optimal
        output while making minimal assumptions about the objective function. Crucially, BO does not require the
        function to be differentiable. This is a significant advantage in synthetic biology, where response landscapes
        are frequently rugged, discontinuous, or stochastic due to complex molecular interactions, making gradient-based
        optimisation methods inapplicable<sup>[2]</sup>. By assuming only continuity<sup>[1]</sup>, BO is well-suited to
        navigate these
        complex, unpredictable biological systems where the underlying mechanisms are often intractable.
      </p>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        In biological research, the experimental landscape is often complex and high-dimensional. Traditional methods
        like grid search become intractable due to the "curse of dimensionality," where the number of experiments
        required grows exponentially with the number of parameters. Simpler algorithms, such as one-factor-at-a-time
        searches (a form of gradient descent), can easily get trapped in local optima when the system does not behave as
        expected, thus requiring an arbitrary number of restarts to discover the global optimum. BO is engineered to
        navigate these challenges, performing effectively in scenarios with up to 20 input dimensions, and can easily
        handle more with some tuning<sup>[4]</sup>.
      </p>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        The power of BO stems from three core components<sup>[1],[3]</sup>:
      </p>

      <ul style="text-align: left; padding-left: 1.5rem; margin-bottom: 1.5rem;">
        <li><strong>Bayesian inference</strong> to update beliefs based on evidence.</li>
        <li>A <strong>Gaussian Process (GP)</strong> to create a probabilistic model of the function.
        </li>
        <li>An <strong>Acquisition function</strong> to intelligently balance the exploration-exploitation trade-off.
        </li>
      </ul>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        For comprehensive mathematical background, see Roman Garnett's <em>Bayesian Optimization</em> (2023).
        <sup>[1]</sup>
      </p>

      <!-- SUBSECTION: The Bayesian Approach -->
      <h3 style="text-align: left; margin-top: 2rem;">The Bayesian Approach: Learning from Data</h3>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        True to its name, BO is founded on Bayesian statistics. Unlike frequentist methods that provide single-point
        estimates, the Bayesian approach models the entire probability distribution of possible outcomes. This method
        preserves information by propagating the complete underlying distributions through calculations, which is
        critical when dealing with costly and often noisy biological data. A key feature is the ability to incorporate
        prior knowledge (a "prior") into the model, which is then updated with new experimental data to form a more
        informed distribution (a "posterior"). This iterative updating is ideal for lab-in-a-loop biological research,
        where each data point is expensive to acquire and system noise can be unpredictable
        (heteroscedastic)<sup>[1],[3]</sup>.
      </p>

      <!-- SUBSECTION: The Gaussian Process -->
      <h3 style="text-align: left; margin-top: 2rem;">The Gaussian Process: A Probabilistic Map of the Landscape</h3>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        The second component, the Gaussian Process (GP), serves as a probabilistic surrogate model for the black-box
        function. A GP defines a distribution over functions; for any set of input parameters, it returns a Gaussian
        distribution of the expected output, characterised by a mean and a variance. This provides not just a prediction
        but also a measure of uncertainty for that prediction. Central to the GP is the covariance function, or kernel,
        which encodes assumptions about the function's smoothness and shape. The kernel defines how related the outputs
        are for different inputs, allowing the GP to generalise from observed data to unexplored regions of the
        parameter space. A well-chosen kernel is crucial for balancing the risks of overfitting (mistaking noise for a
        real trend) and underfitting (missing a genuine trend in the data), a common challenge with inherently noisy
        biological datasets[1],[3]</sup>.
      </p>

      <!-- SUBSECTION: The Acquisition Function -->
      <h3 style="text-align: left; margin-top: 2rem;">The Acquisition Function: Balancing Exploration and Exploitation
      </h3>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        The GP model, with its predictions of mean and variance, guides the search for the next set of parameters to
        test experimentally. This guidance is formalised by the acquisition function. This function calculates the
        expected "utility" of evaluating each point in the parameter space, effectively balancing the trade-off between
        exploring uncertain regions and exploiting areas known to yield good results.
      <ul style="text-align: left; padding-left: 1.5rem; margin-bottom: 1.5rem;">
        <li><strong>Exploitation</strong> involves sampling in regions where the GP predicts a high mean value, refining
          our knowledge around known optima..</li>
        </li><strong>Exploration</strong> involves sampling in regions where the GP predicts high variance,
        </li>
      </ul>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        The next experimental point is chosen by finding the parameters that maximise the acquisition function. This
        dynamic approach ensures that the search efficiently converges toward the global optimum by focusing resources
        on promising regions while avoiding wasteful experiments in poorly performing ones. Common acquisition functions
        include Probability of Improvement (PI), Expected Improvement (EI), and Upper Confidence Bound
        (UCB)<sup>[1][3]</sup>.
      </p>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        This trade-off can be further tuned by adopting a risk-averse or risk-seeking policy, often by adjusting a
        parameter within the acquisition function. A risk-averse strategy prioritises regions promising a certain but
        possibly lower improvement, which is useful when the cost of a failed experiment is high. Conversely, a
        risk-seeking strategy favours more uncertain regions that might result in higher overall improvement. This often
        results in the policy shifting towards exploitation or exploration.
      </p>

      <!-- SUBSECTION: The Optimisation Workflow -->
      <h3 style="text-align: left; margin-top: 2rem;">The Optimisation Workflow</h3>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        By integrating these concepts, BO can identify an optimal set of parameters with a minimal number of
        experimental iterations. The typical workflow is as follows:
      </p>

      <ol style="text-align: left; padding-left: 1.5rem; margin-bottom: 1.5rem;">
        <li><strong>Initialisation:</strong> Begin with a small set of initial data points, either from prior knowledge
          or quasi-random sampling (e.g. SOBOL algorithm).</li>
        <li><strong>Model Fitting:</strong> Fit a Gaussian Process surrogate model to the existing data.</li>
        <li><strong>Acquisition:</strong> use the acquisition function to choose the next experiment.</li>
        <li><strong>Experimentation:</strong> run the experiment and record outputs.</li>
        <li><strong>Update:</strong> Add the new data point to the dataset and repeat from step 2 until an
          optimal solution is found or the experimental budget is exhausted.
        </li>
      </ol>

      <p style="text-align: left; margin-bottom: 1.5rem;">
        This process can also be adapted for batch optimisation, where multiple points are suggested for parallel
        evaluation in each cycle. While this can slightly reduce sample efficiency, it significantly accelerates the
        discovery process when multiple experiments can be run concurrently<sup>[3]</sup>.
      </p>

    </div>
  </details>
  <!-- DROPDOWN END -->
</div>
<!-- WHY IS BAYESIAN OPTIMISATION UNDER-EMPLOYED IN BIOLOGY -->
<div class="page-section width-75" style="margin-left: 220px; margin-bottom: 3rem;" id="why-underemployed">
  <h2 style="text-align: left;">
    <span class="span-lime-green" class="animate-text-scramble"><span>Why is Bayesian Optimisation Under-Employed in
        Biology?</span>
  </h2>

  <!-- DROPDOWN START -->
  <details style="margin-top: 1rem; margin-bottom: 2rem;">
    <summary style="cursor: pointer; font-weight: bold;">Show Details</summary>
    <div style="margin-top: 1rem;">

      <p style="text-align: left; margin-bottom: 1.5rem;">
        In the past decade BO has received a significant boost in attention. This can be attributed to a massive leap in
        popularity of machine learning (ML) and the need for computationally expensive hyperparameter
        tuning<sup>[9]</sup>. Almost
        all conventional ML algorithms (linear regression, decision trees, random forests, etc.) have hyper-parameters
        that control how the model is built. For example, the number of unique leaves and branches has a significant
        impact on how well a decision tree generalises data<sup>[10]</sup>. Although rules of thumb for such situations
        exist, they
        often provide non-ideal model performance. The cost of retraining multiple models with varied parameters and
        choosing the best performer is oftentimes much lower than the unrealised gains from deploying a subpar model for
        real life scenarios<sup>[3]</sup>. Consequently multiple BO libraries have been developed for Python (it being
        the de facto
        language for ML). These range from simple BO models implemented within the Sci-Kit package, general
        plug-and-play ML BO (e.g. Optuna)<sup>[11]</sup>, or researcher aimed low-level packages, such as BoTorch (part
        of the
        popular PyTorch package)<sup>[12]</sup> and Ax<sup>[13]</sup>. However, since all of these were developed with
        ML scientists and
        software developers in mind, they require strong programming knowledge to use effectively. Unfortunately, such
        skillset is quite often neglected within the biological sciences apart from bioinformaticians and PhD level
        researchers.
      </p>
      <p style="text-align: left; margin-bottom: 1.5rem;">
        Nevertheless, significantly modified BO has been repeatedly and successfully applied in the context of natural
        sciences. Various BO implementations (often using a Kernel assuming some level of white noise) have been used in
        academia and in the corporate sector, with Ax and BoTorch both being co-developed by large companies like
        Meta<sup>[13]</sup>. These applications are more prominent for situations where the signal to noise ratio can be
        sufficiently maximised by use of technical replicates, for instance Material Science<sup>[14],[15]</sup> or
        Chemistry
        research<sup>[16],[17]</sup>. There are even some claims within the research community that technical replicates
        are not
        needed for BO workflows since custom models are able to accommodate some level of noise<sup>[7]</sup>.
      </p>
      <p>We postulate that it is due to the following reasons that BO has been under-appreciated by the biological
        sciences outside of media optimisation and research-level exploration of large genetic combinatorial
        libraries<sup>[18],[20]</sup>. </p>

      <p>One reason is that by itself BO is unintuitive and could be assumed to be similar to a random search. This is a
        misconception though, since non Thompson sampling implementations are completely deterministic and random search
        algorithms actually outperform the more conventional grid search<sup>[3]</sup>. </p>

      <p>The second issue pertains to the high, often heteroscedastic (not-constant along the prediction variable),
        noise levels that come with wet-lab experimentation. Technical replicates are an inherent part of the field and
        provide information not just through discovering the mean but the variance associated with replication. This is
        a significant limitation for the simplest BO models, since they treat the mean of technical replicates as a
        noiseless observation (the ground truth)<sup>[1]</sup>. Consequently, the researcher is required to familiarise
        themselves with research-level implementations that were often designed for solving a specific issue and cannot
        be described as beginner friendly.</p>

      <p>Our dry lab software project aims to address two main points of the issue. First, it acts as an interactive
        playground that would allow an individual to convince themselves of the merits provided by BO. This acts as a
        first stepping stone for anyone who would then go out and implement their own BO workflow to tackle a specific
        real world problem. Second, it has all the necessary functions to run BO as part of an experiment without having
        to write code. Once deployed, the applet can support rapid access and result analysis while in the lab. This is
        enhanced by a core modular functionality, meaning that more engaged users can upload custom acquisition
        functions and kernels to suit their specific needs. </p>



      </p>

    </div>
  </details>
  <!-- DROPDOWN END -->
</div>
<!-- OUTLOOK -->
<div class="page-section width-75" style="margin-left: 220px; margin-bottom: 3rem;" id="outlook">
  <h2 style="text-align: left;">
    <span class="span-lime-green" class="animate-text-scramble"><span>Outlook</span>
  </h2>

  <p style="text-align: left; margin-bottom: 1.5rem;">
    Beyond what we managed to add ahead of the freeze, we are actively engaged in enhancing this package by adding
    greater functionality. Currently, we plan to add the following elements:
  </p>


  <ol style="text-align: left; padding-left: 1.5rem; margin-bottom: 1.5rem;">
    <li>A larger set of kernels, also suitable to exploring mixed-integer spaces (i.e. with continuous and discrete
      input features).</li>
    <li>A flowchart guiding less technically apt users to the correct choices of acquisition functions and kernels based
      on simulations and real-world data. </li>
    <li>Optimising for ‘just-in-time’ induction: some literature[21] indicates that there may be unrealised gains in
      offsetting the induction of some elements of a metabolic pathway, thus we are presently testing ways to jointly
      optimise this</li>
    <li>In its current iteration, the package does not possess any surrogate models like kinetic models to further
      reduce iterations with a mechanistically informed prior, since this still lacks straightforward heuristics for
      implementation. </li>
  </ol>


  <p style="text-align: left; margin-bottom: 1.5rem;">
    We are open to collaborating with future iGEMers to make this a robust and usable package for the community at large. 
  </p>

    <strong>We actively welcome collaboration with future iGEM teams</strong> to turn BioKernel into a robust,
    accessible tool
    that empowers experimentalists to use principled optimisation methods without coding expertise.
  </p>
</div>
<!-- REFERENCES -->
<div class="page-section width-75" style="margin-left: 220px; margin-bottom: 3rem;" id="references">
  <h2 style="text-align: left;">
    <span class="animate-text-scramble">References</span>
  </h2>

  <ol style="text-align: left; padding-left: 1.5rem; line-height: 1.6;">
    <li>
      Brochu, E., Cora, V. M., & De Freitas, N. (2010). <em>A Tutorial on Bayesian Optimization of Expensive Cost
        Functions,
        with Application to Active User Modeling and Hierarchical Reinforcement Learning.</em> arXiv preprint
      arXiv:1012.2599.
    </li>
    <li>
      Garnett, R. (2023). <em>Bayesian Optimization.</em> Cambridge University Press.
    </li>
    <li>
      Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2016).
      <em>Taking the Human Out of the Loop: A Review of Bayesian Optimization.</em>
      Proceedings of the IEEE, 104(1), 148–175.
    </li>
    <li>
      Snoek, J., Larochelle, H., & Adams, R. P. (2012).
      <em>Practical Bayesian Optimization of Machine Learning Algorithms.</em>
      Advances in Neural Information Processing Systems, 25.
    </li>
    <li>
      Meyer, A. J., Segall-Shapiro, T. H., Glassey, E., Zhang, J., & Voigt, C. A. (2019).
      <em>Escherichia coli “Marionette” strains with 12 highly optimized small-molecule sensors.</em>
      Nature Chemical Biology, 15(2), 196–204.
    </li>
    <li>
      Morshed, N., & Siedlecki, M. (2021).
      <em>Bayesian Optimization for Bioprocess Parameter Estimation.</em>
      Biotechnology and Bioengineering, 118(10), 3820–3831.
    </li>
    <li>
      Angermueller, C., et al. (2020).
      <em>Population-based black-box optimisation for biological design.</em>
      Nature Communications, 11, 5280.
    </li>
    <li>
      Lappalainen, J., & Rousu, J. (2022).
      <em>Gaussian Processes for Synthetic Biology Design.</em>
      Synthetic Biology Journal, 7(4), ysac018.
    </li>
    <li>
      Gómez-Bombarelli, R., et al. (2018).
      <em>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules.</em>
      ACS Central Science, 4(2), 268–276.
    </li>
    <li>
      Reuel, N. F., & Jensen, K. F. (2022).
      <em>Bringing Statistical Optimisation into Experimental Biology.</em>
      Trends in Biotechnology, 40(3), 258–269.
    </li>
  </ol>
</div>

{% endblock %}